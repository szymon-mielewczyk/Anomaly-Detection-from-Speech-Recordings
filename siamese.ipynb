{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.manifold import TSNE\n",
    "import lightning as L\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import sklearn\n",
    "print(librosa.__version__)\n",
    "print(torch.__version__)\n",
    "print(matplotlib.__version__)\n",
    "print(sklearn.__version__)\n",
    "print(L.__version__)\n",
    "print(torchmetrics.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "class Audio_dataset(Dataset):\n",
    "    def __init__(self, dir_path, dataset_sampling_rate, anomaly_dir_path):\n",
    "        # self.dataset_sampling_rate = 22050\n",
    "        self.ANOMALY_POWER = 1\n",
    "        self.dir_path = dir_path\n",
    "        self.anomaly_dir_path = anomaly_dir_path\n",
    "        self.dataset_sampling_rate = dataset_sampling_rate\n",
    "        self.file_names = os.listdir(self.dir_path)\n",
    "        if 'desktop.ini' in self.file_names:\n",
    "            self.file_names.remove('desktop.ini')\n",
    "        self.last_idx = len(self.file_names)\n",
    "        self.anomaly_recorded_path_dir = self.anomaly_dir_path\n",
    "        self.anomaly_recorded_file_names = os.listdir(self.anomaly_recorded_path_dir)\n",
    "        if 'desktop.ini' in  self.anomaly_recorded_file_names:\n",
    "             self.anomaly_recorded_file_names.remove('desktop.ini')\n",
    "        self.anomaly_speech_path_dir =  self.dir_path\n",
    "        self.anomaly_speech_file_names = os.listdir(self.anomaly_speech_path_dir)\n",
    "        if 'desktop.ini' in  self.anomaly_speech_file_names:\n",
    "             self.anomaly_speech_file_names.remove('desktop.ini')\n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, sr, cqt_1 = self.pipeline(idx)\n",
    "        _, _, cqt_2 = self.pipeline(random.randint(0, self.last_idx - 1), anomaly=True)\n",
    "        _, _, anchor = self.pipeline(random.randint(0, self.last_idx - 1))\n",
    "        return anchor, cqt_1, cqt_2, data, sr\n",
    "\n",
    "    def pipeline(self, id, anomaly=False):\n",
    "        data, sr = self.load_data(id)\n",
    "        data = self.cut_silence(data)\n",
    "        data = self.augment_audio(data, sr)\n",
    "        data = self.cut_data(data)\n",
    "        if anomaly:\n",
    "            data = self.add_anomaly(data, sr)\n",
    "        cqt  = self.make_cqt(data)\n",
    "        cqt = np.abs((cqt+80)/80)\n",
    "\n",
    "        # augment cqt do przetestowania\n",
    "        # cqt = self.augment_cqt(cqt)\n",
    "\n",
    "        cqt = cqt.reshape((1, cqt.shape[0], cqt.shape[1]))\n",
    "        return data, sr, cqt\n",
    "    \n",
    "    def load_data(self,id):\n",
    "        file_path = os.path.join(self.dir_path, self.file_names[id])\n",
    "        data, sr = librosa.load(file_path, sr=self.dataset_sampling_rate)\n",
    "        return data, sr\n",
    "\n",
    "    def cut_silence(self, data):\n",
    "        data_intervals = librosa.effects.split(y=data,frame_length=512, hop_length=128, top_db=48)\n",
    "        data = np.concatenate([data[s[0]:s[1]] for s in data_intervals])\n",
    "        return data\n",
    "\n",
    "    def augment_audio(self, data, sr):\n",
    "        steps = (random.random() * 4) - 2\n",
    "        stretch = 1 - (random.random() * 0.2)\n",
    "        data = librosa.effects.time_stretch(data, rate=stretch)\n",
    "        data = librosa.effects.pitch_shift(data, sr=sr, n_steps=steps)\n",
    "        return data\n",
    "\n",
    "    def cut_data(self, data):\n",
    "        # the last possible element to slice\n",
    "        end_point = len(data) - self.dataset_sampling_rate\n",
    "        # choose first elemnt \n",
    "        start_point = random.randint(0, end_point)\n",
    "        # slice 1s of data\n",
    "        data = data[start_point:start_point + self.dataset_sampling_rate]\n",
    "        return data\n",
    "\n",
    "    def make_cqt(self, data):\n",
    "        C = librosa.cqt(data, sr=self.dataset_sampling_rate)\n",
    "        cqt = librosa.power_to_db(np.abs(C)**2)\n",
    "        cqt = np.array(cqt, dtype='float32')\n",
    "        return cqt\n",
    "    \n",
    "    def augment_cqt(self,data):\n",
    "        flip = random.random()\n",
    "        if flip > 0.8:\n",
    "            data = np.flip(data,axis=1)\n",
    "        return data\n",
    "    \n",
    "    def add_anomaly(self, data, sr):\n",
    "        anomaly_type = random.randint(0,1)\n",
    "        if anomaly_type == 0:\n",
    "            anomaly_recorded_id = random.randint(0, len(self.anomaly_recorded_file_names)-1)\n",
    "            anomaly_file_path = os.path.join(self.anomaly_recorded_path_dir, self.anomaly_recorded_file_names[anomaly_recorded_id])\n",
    "            anomaly_file_data_raw, _ = librosa.load(anomaly_file_path, sr=sr)\n",
    "            max_starting_sample = len(data) - len(anomaly_file_data_raw)\n",
    "            start = random.randint(0, max_starting_sample-1)\n",
    "            for i in range(len(anomaly_file_data_raw)):\n",
    "                data[start] += anomaly_file_data_raw[i]\n",
    "                start += 1\n",
    "        elif anomaly_type == 1:\n",
    "            anomaly_recorded_id = random.randint(0, len(self.anomaly_speech_file_names)-1)\n",
    "            anomaly_file_path = os.path.join(self.anomaly_speech_path_dir, self.anomaly_speech_file_names[anomaly_recorded_id])\n",
    "            anomaly_file_data_raw, _ = librosa.load(anomaly_file_path, sr=sr)\n",
    "            anomaly_data_intervals = librosa.effects.split(y=anomaly_file_data_raw,frame_length=512, hop_length=128, top_db=48)\n",
    "            anomaly_data = np.concatenate([anomaly_file_data_raw[s[0]:s[1]] for s in anomaly_data_intervals])\n",
    "\n",
    "            anomaly_start_sample = random.randint(0, int(sr/2)-1)\n",
    "            anomaly_length = random.randint(int(sr/4), int(sr/2)-1)\n",
    "            anomaly_data = anomaly_data[anomaly_start_sample: anomaly_start_sample + anomaly_length]\n",
    "\n",
    "            max_starting_sample = len(data) - len(anomaly_data)\n",
    "            start = random.randint(0, max_starting_sample-1)\n",
    "            for i in range(len(anomaly_data)):\n",
    "                anomaly_data[i] *= self.ANOMALY_POWER\n",
    "                data[start] += anomaly_data[i]\n",
    "                start += 1\n",
    "                \n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "class Audio_conv(nn.Module):\n",
    "    def __init__(self, margin, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 1, 'same')\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(16, 64, 3, 1, 'same')\n",
    "        self.conv3 = nn.Conv2d(64, 64, 5, 1, 'same')\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3, 1, 'same')\n",
    "        \n",
    "        self.self_attn = nn.MultiheadAttention(128, 16, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(6400, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 32)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self,x):\n",
    "        #conv\n",
    "        x_conv  = self.conv1(x)\n",
    "        x_conv  = F.leaky_relu(x_conv)\n",
    "        x_conv  = self.pool(x_conv )\n",
    "\n",
    "        x_conv  = self.conv2(x_conv )\n",
    "        x_conv  = F.leaky_relu(x_conv )\n",
    "        x_conv  = self.pool(x_conv )\n",
    "\n",
    "        x_conv  = self.conv3(x_conv )\n",
    "        x_conv  = F.leaky_relu(x_conv )\n",
    "\n",
    "        x_conv  = self.conv4(x_conv )\n",
    "        x_conv  = F.leaky_relu(x_conv )\n",
    "        x_conv  = self.pool(x_conv )\n",
    "\n",
    "        #attention\n",
    "        x_conv  = x_conv.permute(0, 2, 3, 1)\n",
    "        batch_size, height, width, feature_dim = x_conv.size()\n",
    "        x_conv = x_conv.reshape(batch_size, height * width, feature_dim)\n",
    "        attn_output, _ = self.self_attn(x_conv, x_conv, x_conv)\n",
    "        attn_output = attn_output.reshape(batch_size, height, width, feature_dim)\n",
    "        attn_output = attn_output.permute(0, 3, 1, 2)\n",
    "\n",
    "        #fully connected\n",
    "        x_fc = torch.flatten(attn_output, 1)\n",
    "        x_fc = self.fc1(x_fc)\n",
    "        x_fc = F.relu(x_fc)\n",
    "        x_fc = self.dropout(x_fc)\n",
    "        x_fc = self.fc2(x_fc)\n",
    "        x_fc = F.relu(x_fc)\n",
    "        x_fc = self.dropout(x_fc)\n",
    "        x_fc = self.fc3(x_fc)\n",
    "        x_fc = F.relu(x_fc)\n",
    "        x_fc = self.dropout(x_fc)\n",
    "        x_fc = self.fc4(x_fc)\n",
    "\n",
    "        return x_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lit_wrapper(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = torch.nn.TripletMarginLoss()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x1, x2, x3, _, _ = batch\n",
    "        output1 = self.model(x1)\n",
    "        output2 = self.model(x2)\n",
    "        output3 = self.model(x3)\n",
    "        loss = self.loss_fn(output1,output2,output3)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def on_test_start(self):\n",
    "         self.test_step_outputs = []\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        _, x1, x2, _, _ = batch\n",
    "        output1 = self.model(x1)\n",
    "        output2 = self.model(x2)\n",
    "        self.test_step_outputs.append(output1)\n",
    "        self.test_step_outputs.append(output2)\n",
    "    \n",
    "    def on_test_end(self):\n",
    "        self.test_outputs = torch.stack(self.test_step_outputs)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        x1, x2, x3, _, _ = batch\n",
    "        output1 = self.model(x1)\n",
    "        output2 = self.model(x2)\n",
    "        output3 = self.model(x3)\n",
    "        loss = self.loss_fn(output1,output2,output3)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def loss_func(self, output1, output2):\n",
    "        loss, pos_accu, neg_accu= 0, 0, 0\n",
    "        size = len(output1)\n",
    "        for i in range(0, size, 2):\n",
    "            pos_dis = output1[i] - output1[i+1]\n",
    "            pos = torch.linalg.vector_norm(pos_dis)\n",
    "            pos_accu += pos\n",
    "            neg_dis = output2[i] - output2[i+1]\n",
    "            neg = torch.linalg.vector_norm(neg_dis)\n",
    "            neg_accu += neg\n",
    "            partial_losses = pos + np.maximum(0, self.model.margin - neg.item())\n",
    "            loss += torch.mean(partial_losses)\n",
    "        return loss, pos_accu, neg_accu\n",
    "    \n",
    "    def loss_func_euclidean(self, output1, output2):\n",
    "        loss, pos_accu, neg_accu= 0, 0, 0\n",
    "        size = len(output1)\n",
    "        for i in range(0, size, 2):\n",
    "            pos_dis = F.pairwise_distance(output1[i], output1[i+1])\n",
    "            neg_dis = F.pairwise_distance(output2[i], output2[i+1])\n",
    "            loss += pos_dis + np.maximum(0, self.model.margin - neg_dis.item())\n",
    "            pos_accu += pos_dis\n",
    "            neg_accu += neg_dis\n",
    "        return loss, pos_accu, neg_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "checkpoint_callback = L.pytorch.callbacks.ModelCheckpoint(\n",
    "    dirpath='models/top',\n",
    "    filename='{epoch}-{val_loss:.2f}-',\n",
    "    monitor='val_loss',\n",
    "    save_top_k = 1\n",
    ")\n",
    "\n",
    "#early stipping\n",
    "early_stopping_callback = L.pytorch.callbacks.early_stopping.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "#train\n",
    "#dir_path = r\"..\\datasets\\kaggle\\LJ_Speech_dataset\\archive\\LJSpeech-1.1\\wavs\"\n",
    "dir_path = r\"..\\datasets\\kaggle\\Merged\"\n",
    "anomaly_path = r\"..\\datasets\\anomalies\\nagrane\"\n",
    "sr = 22050\n",
    "train = Audio_dataset(dir_path, sr, anomaly_dir_path=anomaly_path)\n",
    "train_dataloader = DataLoader(train, batch_size=4, shuffle=True)\n",
    "\n",
    "#val and test\n",
    "dir_path = r\"..\\datasets\\kaggle\\merged_2_speakers_test\"\n",
    "anomaly_path = r\"..\\datasets\\anomalies\\nagrane_test\"\n",
    "sr = 22050\n",
    "test_dataset = Audio_dataset(dir_path, sr, anomaly_dir_path=anomaly_path)\n",
    "val_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "#debug\n",
    "test, debug = torch.utils.data.random_split(test_dataset, [0.99, 0.01])\n",
    "debug_dataloader = DataLoader(debug, batch_size=1, shuffle=False)\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=30, \n",
    "    # gradient_clip_val=1, \n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    # check_val_every_n_epoch=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_conv = Audio_conv(margin=1)\n",
    "model = Lit_wrapper(audio_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.test_outputs.cpu()\n",
    "output = torch.squeeze(output)\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "components = tsne.fit_transform(output)\n",
    "\n",
    "labels = np.zeros(len(output))\n",
    "for i in range(len(output)):\n",
    "    if i % 2 == 0:\n",
    "        labels[i] = 1\n",
    "    else:\n",
    "        labels[i] = 0\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(components[:, 0], components[:, 1], cmap='viridis', c=labels)\n",
    "plt.colorbar()\n",
    "plt.title('t-SNE Visualization of Model Output')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot 1 class\n",
    "labels = np.zeros(len(output))\n",
    "data1 = np.zeros((int(len(output)/2),2))\n",
    "data2 = np.zeros((int(len(output)/2),2))\n",
    "i1 = 0\n",
    "i2 = 0\n",
    "for i in range(len(output)):\n",
    "    if i % 2 == 0:\n",
    "        data1[i1] = components[i]\n",
    "        i1 += 1\n",
    "    else:\n",
    "        data2[i2] = components[i]\n",
    "        i2 += 1\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(data1[:, 0], data1[:, 1], cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('t-SNE Visualization of Model Output')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot 2 class\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(data2[:, 0], data2[:, 1], cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('t-SNE Visualization of Model Output')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.fc = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_backbone= self.model(x)\n",
    "        x = self.fc(x_backbone)\n",
    "        x = F.sigmoid(x)\n",
    "        return x, x_backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector_wrapper(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.score = []\n",
    "        self.f1 = torchmetrics.F1Score(task='multiclass', num_classes=2)\n",
    "\n",
    "    def on_train_start(self):\n",
    "        self.tensors_train = []\n",
    "        self.labels_train = []\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        _, x1, x2, _, _ = batch\n",
    "        xs = []\n",
    "        labels = []\n",
    "        for i in range(len(x1)):\n",
    "            is_anomaly = random.randint(0,1)\n",
    "            if is_anomaly == 0:\n",
    "                x = x1[i]\n",
    "                label = torch.Tensor([1,0])\n",
    "            elif is_anomaly == 1:\n",
    "                x = x2[i]\n",
    "                label = torch.Tensor([0,1])\n",
    "            xs.append(x)\n",
    "            labels.append(label)\n",
    "        x = torch.stack(xs)\n",
    "        label = torch.stack(labels).to('cuda:0')\n",
    "        output, backbone = self.model(x)\n",
    "        self.tensors_train.append(backbone)\n",
    "        self.labels_train.append(label)\n",
    "        loss = self.loss_fn(output, label)\n",
    "        self.log(\"train_detector_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_end(self):\n",
    "        self.train_tensors_end = torch.stack((self.tensors_train))\n",
    "        self.train_labels_end = torch.stack((self.labels_train))\n",
    "\n",
    "    def train_step_debug(self,batch):\n",
    "        x1, x2 = batch\n",
    "        xs = []\n",
    "        labels = []\n",
    "        for i in range(len(x1)):\n",
    "            is_anomaly = random.randint(0,1)\n",
    "            if is_anomaly == 0:\n",
    "                x = x1[i]\n",
    "                label = torch.Tensor([1,0])\n",
    "            elif is_anomaly == 1:\n",
    "                x = x2[i]\n",
    "                label = torch.Tensor([0,1])\n",
    "            xs.append(x)\n",
    "            labels.append(label)\n",
    "        x = torch.stack(xs)\n",
    "        label = torch.stack(labels).to('cuda:0')\n",
    "        output, backbone = self.model(x)\n",
    "        return backbone\n",
    "\n",
    "    def on_test_start(self):\n",
    "         self.score = []\n",
    "         self.outputs_tensors = []\n",
    "         self.outputs_target = []\n",
    "         self.outputs_preds = []\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        _, x1, x2, _, _ = batch\n",
    "        xs = []\n",
    "        labels = []\n",
    "        for i in range(len(x1)):\n",
    "            if random.randint(0,1) == 0:\n",
    "                x = x1[i]\n",
    "                label = torch.Tensor([1,0])\n",
    "            else:\n",
    "                x = x2[i]\n",
    "                label = torch.Tensor([0,1])\n",
    "            xs.append(x)\n",
    "            labels.append(label)\n",
    "        x = torch.stack(xs)\n",
    "        label = torch.stack(labels).to('cuda:0')\n",
    "        output, _ = self.model(x)\n",
    "        preds = torch.argmax(output,dim=1)\n",
    "        target = torch.argmax(label,dim=1)\n",
    "        score = self.f1(preds, target)\n",
    "        self.score.append(score)\n",
    "        self.outputs_tensors.append(output)\n",
    "        self.outputs_preds.append(preds)\n",
    "        self.outputs_target.append(target)\n",
    "\n",
    "    def on_test_end(self):\n",
    "        self.test_outputs = torch.stack(self.score)\n",
    "        self.test_outputs_tensors = torch.stack(self.outputs_tensors)\n",
    "        self.outputs_preds = torch.stack(self.outputs_preds)\n",
    "        self.outputs_target = torch.stack(self.outputs_target)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        _, x1, x2, _, _ = batch\n",
    "        xs = []\n",
    "        labels = []\n",
    "        for i in range(len(x1)):\n",
    "            is_anomaly = random.randint(0,1)\n",
    "            if is_anomaly == 0:\n",
    "                x = x1[i]\n",
    "                label = torch.Tensor([1,0])\n",
    "            elif is_anomaly == 1:\n",
    "                x = x2[i]\n",
    "                label = torch.Tensor([0,1])\n",
    "            xs.append(x)\n",
    "            labels.append(label)\n",
    "        x = torch.stack(xs)\n",
    "        label = torch.stack(labels).to('cuda:0')\n",
    "        output, backbone = self.model(x)\n",
    "        loss = self.loss_fn(output, label)\n",
    "        self.log(\"val_detector_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = Detector(audio_conv)\n",
    "\n",
    "# train last layer\n",
    "layers_count = 0\n",
    "for param in detector.parameters():\n",
    "    layers_count += 1\n",
    "\n",
    "for i, param in enumerate(detector.parameters()):\n",
    "    if layers_count - i != 1 and layers_count - i != 2:\n",
    "        param.requires_grad = False\n",
    "\n",
    "anomaly_detector = Detector_wrapper(detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in anomaly_detector.parameters():\n",
    "    print(param.shape, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = L.pytorch.callbacks.early_stopping.EarlyStopping(\n",
    "    monitor='val_detector_loss',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "trainer_detector = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    # callbacks=[early_stopping_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_detector.fit(anomaly_detector, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_detector.test(anomaly_detector, test_dataloader)\n",
    "f1 = torch.mean(anomaly_detector.test_outputs).item()\n",
    "print('f1 = ', f1*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = anomaly_detector.test_outputs_tensors\n",
    "output = torch.squeeze(output)\n",
    "output = output.cpu().numpy()\n",
    "labels = anomaly_detector.outputs_target\n",
    "labels = torch.squeeze(labels)\n",
    "labels = labels.cpu().numpy()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(output[:, 0], output[:, 1], cmap='viridis', c=labels)\n",
    "# plt.scatter(output, output, cmap='viridis', c=labels)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "output = anomaly_detector.train_tensors_end.cpu()\n",
    "output = torch.squeeze(output)\n",
    "output = torch.flatten(output, start_dim=0, end_dim=1)\n",
    "labels = anomaly_detector.train_labels_end.cpu()\n",
    "labels = torch.flatten(labels, start_dim=0, end_dim=1)\n",
    "labels = torch.argmax(labels,dim=1)\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "components = tsne.fit_transform(output)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(components[:, 0], components[:, 1], cmap='viridis', c=labels)\n",
    "plt.colorbar()\n",
    "plt.title('t-SNE Visualization of Model Output')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'models/model_final1.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
